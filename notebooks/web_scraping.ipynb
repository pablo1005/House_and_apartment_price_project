{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Importing libraries\n",
    "from selenium import webdriver # Web browser automation for web scraping and automated testing.\n",
    "from selenium.webdriver.firefox.service import Service # Browser Service Control for Selenium.\n",
    "from webdriver_manager.firefox import GeckoDriverManager # Automatic Firefox driver management for Selenium.\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions # Run the browser in headless mode or set other preferences.\n",
    "from selenium.webdriver.common.by import By # Locate elements on the web page in different ways, such as by ID, name, class, label, among others.\n",
    "from selenium.webdriver.support.ui import WebDriverWait # Wait until a specific condition is met on the web page, such as an element being present or visible.\n",
    "from selenium.webdriver.support import expected_conditions as EC # Expect certain events to occur on the web page.\n",
    "\n",
    "from bs4 import BeautifulSoup # HTML document analysis  for web scraping.\n",
    "\n",
    "import time # This module provides functions to manage time.\n",
    "\n",
    "import re # Provides functions for working with regular expressions, which are used to find and manipulate patterns in text.\n",
    "\n",
    "import math # Provides standard mathematical functions, such as arithmetic operations and algebra calculations.\n",
    "\n",
    "import pandas as pd # It is a library for data manipulation and analysis in Python.\n",
    "\n",
    "import requests # This module makes it easy to send HTTP requests in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Set houses and apartments price range.\n",
    "lower_price = 400000\n",
    "upper_price = 2000000\n",
    "# Blank list where house and apartments ads will be stored.\n",
    "house_apartment_list = []\n",
    "# Blank list where houses and apartments ads description will be stored.\n",
    "list_description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Web page URL from which the data will be extracted.\n",
    "def get_page_url(page_number,rango_i,rango_s):\n",
    "    \n",
    "    base_page_url = \"https://www.encuentra24.com/guatemala-es/bienes-raices-venta-de-propiedades-casas/guatemala-es-guatemala.{}?q=f_price.{}-{}|f_currency.GTQ|withcat.bienes-raices-venta-de-propiedades-casas,bienes-raices-venta-de-propiedades-apartamentos\".format(page_number,rango_i,rango_s)\n",
    "    \n",
    "    return base_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Create a driver object representing an automatically controlled Firefox web browser.\n",
    "driver = webdriver.Firefox()\n",
    "# Browser window maximize.\n",
    "driver.maximize_window()\n",
    "# Go to the page want to Scrape.\n",
    "driver.get(get_page_url(1,lower_price,upper_price))\n",
    "# Pause to ensure completely page loads. \n",
    "time.sleep(5)\n",
    "# Get HTML code from loaded page.\n",
    "html_code = driver.page_source\n",
    "# Parse HTML with BeautifulSoup.\n",
    "soup = BeautifulSoup(html_code, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Function that allows to extract basic information of each ad (title, price, square meters, \n",
    "# number of rooms, parking spaces and bathrooms, location, image url and ad url).\n",
    "def div_html_to_obj(house_div):\n",
    "    # We first try to find \"data-original\" attribute for the image, if it does not exist, we look for \"src\".\n",
    "    try:\n",
    "        img_url = house_div.find(\"img\")['data-original']\n",
    "    except:\n",
    "        img_url = house_div.find(\"img\")['src']  \n",
    "    \n",
    "    # Extract ad location.\n",
    "    location = house_div.find(class_='d3-ad-tile__location').text.strip()\n",
    "    \n",
    "    # Extract ad title.\n",
    "    title = house_div.find(class_='d3-ad-tile__title').text\n",
    "    \n",
    "    # Get the element that contains price.\n",
    "    price_dirt = house_div.find(class_=\"d3-ad-tile__price\").text\n",
    "    \n",
    "    # Define a regular expression to extract main price.\n",
    "    price_regex = r'([Q$]\\d[\\d,.]+)'\n",
    "    \n",
    "    # Finding price using regular expressions.\n",
    "    original_price = re.findall(price_regex, price_dirt)\n",
    "    \n",
    "    # Get ad attributes text (details).\n",
    "    attributes = house_div.find(class_='d3-ad-tile__details').find_all('li', class_='d3-ad-tile__details-item')\n",
    "    \n",
    "    # Initialize variables for square meters, rooms, parking spaces and bathrooms.\n",
    "    square_meters = None\n",
    "    rooms = None\n",
    "    parking = None\n",
    "    baths = None\n",
    "    \n",
    "    # Iterate over details to extract information.\n",
    "    for attribute in attributes:\n",
    "        icon = attribute.find('svg', class_='d3-icon d3-ad-tile__icon')\n",
    "        value = attribute.text.strip()\n",
    "        \n",
    "        # If find a corresponding icon, we identify the type of detail.\n",
    "        if icon:\n",
    "            icon_use = icon.find('use').get('xlink:href')\n",
    "            \n",
    "            # Compare icon with known types to extract corresponding information.\n",
    "            if 'resize' in icon_use:  # Square meters\n",
    "                square_meters = int(re.search(r'\\d+', value).group())\n",
    "            elif 'bed' in icon_use:  # Rooms\n",
    "                rooms = int(value)\n",
    "            elif 'parking' in icon_use:  # Parking\n",
    "                parking = int(value)\n",
    "            elif 'bath' in icon_use:  # Baths\n",
    "                baths = int(value)\n",
    "    \n",
    "    # Build full ad URL.\n",
    "    url = 'https://www.encuentra24.com' + house_div.find(\"a\")[\"href\"]\n",
    "    \n",
    "    # Return a dictionary with all extracted information.\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"price\": original_price,\n",
    "        \"square_meters\": square_meters,\n",
    "        \"rooms\": rooms,\n",
    "        \"parking\": parking,\n",
    "        \"baths\": baths,\n",
    "        \"location\": location,\n",
    "        \"img_url\": img_url,\n",
    "        \"url\": url\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_and_save_items(page_number):\n",
    "    # Navigate to specified page number and retrieve HTML source.\n",
    "    driver.get(page_number)\n",
    "    html_code = driver.page_source\n",
    "    \n",
    "    # Parse HTML using BeautifulSoup.\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    \n",
    "    # Find all house and apartments advertisement 'div' elements.\n",
    "    all_house_div = soup.find_all(\"div\", class_=\"d3-ad-tile\")\n",
    "    \n",
    "    # Iterate over each house div, parse its content using div_html_to_obj, and append the result to house_apartment_list.\n",
    "    for house_div in all_house_div:\n",
    "        # Convert HTML div element to a dictionary object using 'div_html_to_obj' function.\n",
    "        a = div_html_to_obj(house_div)\n",
    "        # Append dictionary object to house_apartment_list.\n",
    "        house_apartment_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Uses Selenium to navigate to a specific page, capture the HTML code, and then uses BeautifulSoup to parse it.\n",
    "def web_page_number():\n",
    "    # Extract ads total number and ads per page from HTML.\n",
    "    # Position 1 shows total ads number, and position 5 shows ads number per page.    \n",
    "    total_number_ads = int(soup.find(class_ = \"d3-category-list__results\").text.replace(\",\",\"\").split()[5])\n",
    "    total_ads_page = int(soup.find(class_ = \"d3-category-list__results\").text.split()[3])\n",
    "    \n",
    "    # Calculate pages total number needed, rounding up to the nearest integer.\n",
    "\n",
    "    division = float(total_number_ads/total_ads_page)\n",
    "    \n",
    "    page_amount = math.ceil(division)\n",
    "    \n",
    "    return page_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Extracts ads total number and web page ads per page, calculates total pages number needed, rounding up to the nearest integer.\n",
    "def web_page_number():\n",
    "    # Extract ads total number and ads per page from HTML.\n",
    "    # Position 1 shows total ads number, and position 5 shows ads number per page.    \n",
    "    total_number_ads = int(soup.find(class_ = \"d3-category-list__results\").text.replace(\",\",\"\").split()[5])\n",
    "    total_ads_page = int(soup.find(class_ = \"d3-category-list__results\").text.split()[3])\n",
    "    \n",
    "    # Calculate pages total number needed, rounding up to the nearest integer.\n",
    "\n",
    "    division = float(total_number_ads/total_ads_page)\n",
    "    \n",
    "    page_amount = math.ceil(division)\n",
    "    \n",
    "    return page_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Scrape and process property listings from a web page.\n",
    "def parser_and_save_items(page_number):\n",
    "    # Navigate to specified page number using web driver.\n",
    "    driver.get(page_number)\n",
    "    # Get HTML current page code.\n",
    "    html_code = driver.page_source\n",
    "    # Parse HTML code with BeautifulSoup to create a soup object.\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    # Find all 'div' elements class: 'd3-ad-tile' which represent individual property listings.\n",
    "    all_house_div = soup.find_all(\"div\", class_=\"d3-ad-tile\")\n",
    "    \n",
    "    # Iterate over each div element found\n",
    "    for house_div in all_house_div:\n",
    "        # Convert HTML div element to a dictionary object using 'div_html_to_obj' function.\n",
    "        a = div_html_to_obj(house_div)\n",
    "        # Append dictionary object to house_apartment_list.\n",
    "        house_apartment_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Iterate through each page number from 1 to pages total number.\n",
    "for actual_page in range(1, web_page_number() + 1):\n",
    "    # Generate URL for current page with specified price range.\n",
    "    a = get_page_url(actual_page, lower_price, upper_price)\n",
    "\n",
    "    # Scrape and process property listings from URL generated.\n",
    "    parser_and_save_items(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Create DataFrame with basic ad information.\n",
    "df = pd.DataFrame(house_apartment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Scrape property descriptions function.\n",
    "def obtain_description(ad_url):\n",
    "    try:\n",
    "        # Set up HTTP headers to mimic a real web browser request.\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        # Send an HTTP GET request to specified URL headers.\n",
    "        response = requests.get(ad_url, headers=headers)\n",
    "        # Ensure that request was successful; raise an error for unsuccessful status codes.\n",
    "        response.raise_for_status()  \n",
    "\n",
    "        # Parse page HTML content using BeautifulSoup.\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find HTML element containing property description.\n",
    "        description_element = soup.find('div', class_='d3-property-about__text')\n",
    "        \n",
    "        if description_element:\n",
    "            # Extract and clean up the description text.\n",
    "            description = description_element.text.strip().replace(\"\\n\", \" \")\n",
    "            return description\n",
    "        else:\n",
    "            # Print message if description is not found.\n",
    "            print(f\"Description not found in {ad_url}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print an error message if an exception occurs during the request or parsing.\n",
    "        print(f\"Error getting description of {ad_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description not found in https://www.encuentra24.com/guatemala-es/bienes-raices-venta-de-propiedades-apartamentos/vendo-apartamento-en-zona-11-acodi/25598378?q=f_price.400000-2000000|f_currency.GTQ|withcat.bienes-raices-venta-de-propiedades-casas,bienes-raices-venta-de-propiedades-apartamentos&regionslug=guatemala-es-guatemala&list=categoryregion&categoryslug=bienes-raices-venta-de-propiedades-casas\n",
      "Description not found in https://www.encuentra24.com/guatemala-es/bienes-raices-venta-de-propiedades-casas/vendo-casa-en-zona-4-de-mixco-acodi/25598170?q=f_price.400000-2000000|f_currency.GTQ|withcat.bienes-raices-venta-de-propiedades-casas,bienes-raices-venta-de-propiedades-apartamentos&regionslug=guatemala-es-guatemala&list=categoryregion&categoryslug=bienes-raices-venta-de-propiedades-casas\n"
     ]
    }
   ],
   "source": [
    "# 11) Obtain ads URLs that have a defined URL.\n",
    "\n",
    "# Create a URLs list from `house_apartment_list` where 'url' field is defined.\n",
    "ad_urls = [ad[\"url\"] for ad in house_apartment_list if ad[\"url\"]]\n",
    "\n",
    "# Iterate over each URL in the list of URLs.\n",
    "for i in ad_urls:\n",
    "    # Obtain description for each URL ad.\n",
    "    obt_description = obtain_description(i)\n",
    "    # Append obtained description to 'list_description'.\n",
    "    list_description.append(obt_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Add descriptions to DataFrame.\n",
    "df = df.assign(adv_description=list_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Save cleaned DataFrame.\n",
    "\n",
    "# Specify directory path.\n",
    "directory = 'C:\\\\Users\\\\DAV\\\\Documents\\\\Python\\\\Python_Project\\\\House_price_project\\\\data\\\\raw\\\\'\n",
    "# Define the file name.\n",
    "filename = 'raw-data.csv'\n",
    "# Build full file path.\n",
    "output_path = directory + filename\n",
    "# Export DataFrame `df` to an CSV file named 'raw.csv' \n",
    "df.to_csv(output_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
